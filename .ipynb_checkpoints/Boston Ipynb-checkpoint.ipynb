{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download (google colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllBostonData  = pd.read_csv('listings.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AllBostonData.head(10))\n",
    "print(AllBostonData.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing location based columns as well because longitude and latitude are included. \n",
    "columnIndexToRemove  = [i for i in range(0, 26) ]\n",
    "columnIndexToRemove+=( 29, 30, 31, 37, 38, 39, 40, 41, 42, 43,  44, 45, 46, 47, 59, 61, 62, 69,70,  75,77,78, 86, 87, 88  )\n",
    "\n",
    "#pd.options.display.max_columns = None\n",
    "print(\"Dropped data:\")\n",
    "print(AllBostonData.iloc[:,columnIndexToRemove].info())\n",
    "BostonData  = AllBostonData.drop(columns=AllBostonData.iloc[:,columnIndexToRemove].columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New DataFrame Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BostonData.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns that we have to modify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Nan's in the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsToOneHotEncode = ['property_type', 'room_type','bed_type', 'cancellation_policy']\n",
    "columnToScaleAndParse = ['host_response_rate', 'host_acceptance_rate']\n",
    "TrueFalseColumns = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'is_location_exact', 'instant_bookable', 'require_guest_profile_picture', 'require_guest_phone_verification']\n",
    "dollarSignColumns = [\"extra_people\", \"price\", \"security_deposit\", \"cleaning_fee\"]\n",
    "nanIndexDict = {}\n",
    "for column in columnsToOneHotEncode:\n",
    "  \n",
    "  if BostonData[column][BostonData[column].isnull()].empty == False:\n",
    "    print(\"{} has null values\".format(column))\n",
    "    nanIndexDict[column] = BostonData[column][BostonData[column].isnull()].index\n",
    "  #print(BostonData[column][BostonData[column].isnull()].head())\n",
    "for column in nanIndexDict.keys():\n",
    "      for index in nanIndexDict[column]:\n",
    "        # use Placeholder as placeholder.  We don't need to change this value, as it will be encoded into its own category.  \n",
    "        BostonData.loc[index, column] = \"PlaceHolder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot encoding False to 0 or 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrueFalseColumns = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'is_location_exact', 'instant_bookable', 'require_guest_profile_picture', 'require_guest_phone_verification']\n",
    "def to10(xVals):\n",
    "    return 1 if xVals =='t' else 0  \n",
    "\n",
    "for TrueFalseCol in TrueFalseColumns: \n",
    "  #print(EncodedBostonData[TrueFalseCol][EncodedBostonData[TrueFalseCol] == \"t\"])\n",
    "    BostonData[TrueFalseCol] = BostonData[TrueFalseCol].map(to10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### % to decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnToScaleAndParse = ['host_response_rate', 'host_acceptance_rate']\n",
    "def removeAndScale(xVals):\n",
    "    if type(xVals) == float: \n",
    "        return np.nan\n",
    "    newVal =  xVals.replace(\"%\", \"\")\n",
    "    newVal = float(newVal)\n",
    "    newVal = newVal /100  \n",
    "    return newVal\n",
    "for column in columnToScaleAndParse:\n",
    "    BostonData[column] = BostonData[column].map(removeAndScale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( BostonData[\"host_response_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove $$$ signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to check which data to parse\n",
    "#print(EncodedBostonData.info() )\n",
    "dollarSignColumns = [\"extra_people\", \"price\", \"security_deposit\", \"cleaning_fee\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDollar(xVals):\n",
    "    if type(xVals) == float: \n",
    "        return np.nan\n",
    "    newVal = xVals.replace(\"$\", \"\")\n",
    "    if ',' in newVal:\n",
    "        newVal = newVal.replace(\",\", \"\")\n",
    "    newVal = float(newVal)\n",
    "    return newVal\n",
    "for column in dollarSignColumns:\n",
    "  #EncodedBostonData[column] =\n",
    "  BostonData[column] = BostonData[column].map(removeDollar)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BostonData[\"cleaning_fee\"][BostonData[\"cleaning_fee\"]  != np.nan])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move Price to end of DataFrame to complete Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(BostonData)\n",
    "# move the column to head of list using index, pop and insert\n",
    "cols.insert(2000, cols.pop(cols.index('price')))\n",
    "BostonData = BostonData.loc[:, cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before One Hotencoding: EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HeatMap Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "corr = BostonData.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 15))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar graphs of \"Objects\" dtypes with respect to price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "#pd.DataFrame(AllBostonData).dtypes.value_counts().plot(kind='bar', rot=0)\n",
    "categoricalDataFrame = AllBostonData.loc[:,AllBostonData.dtypes == 'object'].columns\n",
    "#These were all objects that we need to check within the BostonData variable/ \n",
    "objectsToCheck = []\n",
    "for value in categoricalDataFrame:\n",
    "    for otherValue in BostonData:\n",
    "        if otherValue in value:\n",
    "            objectsToCheck.append(otherValue)\n",
    "print(len(objectsToCheck))\n",
    "mySet = set(BostonData.columns) - set(objectsToCheck)\n",
    "myInts = list(mySet)\n",
    "print(len(myInts))\n",
    "objectsToCheck.remove('amenities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "fig, ax = plt.subplots(8,2, figsize=(25,30))\n",
    "plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=1.5, wspace=0.4)\n",
    "for variable, subplot in zip(objectsToCheck, ax.flatten()):\n",
    "    subplot.title.set_text(\"Count of {}\".format(variable))\n",
    "    sns.countplot(BostonData[variable], ax=subplot)\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(45)\n",
    "  #subplot.axes.get_xaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.countplot(BostonData['price'] )\n",
    "sns.distplot(BostonData['price'],rug=True, hist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plots of Numerical Values with respect to Price \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "fig, ax = plt.subplots(7,4, figsize=(20,30))\n",
    "for variable, subplot in zip(myInts, ax.flatten()):\n",
    "    sns.scatterplot(x=BostonData[variable],y=BostonData['price'], ax=subplot)\n",
    "    if variable == 'longitude' or variable == 'latitude':  \n",
    "        for label in subplot.get_xticklabels():\n",
    "              label.set_rotation(45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows data with hsitorgrams, but harder to show in presentation.  \n",
    "import seaborn as sns \n",
    "for variable in myInts:\n",
    "    sns.jointplot(x=BostonData[variable],y=BostonData['price'])\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialOneHotEncoding = ['host_verifications', 'Amenities']\n",
    "\n",
    "columnsToOneHotEncode = ['property_type', 'room_type','bed_type', 'cancellation_policy']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMINDER TO USE KERAS FOR ONE HOT ENCODING\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "EncodedBostonData = BostonData\n",
    "EncodedBostonData.loc[:, [*columnsToOneHotEncode]] = BostonData.loc[:, [*columnsToOneHotEncode]].apply(le.fit_transform)\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "for column in columnsToOneHotEncode: \n",
    "    encoded = to_categorical(EncodedBostonData[column])\n",
    "    newLabels = [\"{}{}\".format(column, x ) for x in range(0, len(encoded[0]))] # add labels for each new column\n",
    "    dfToAdd = pd.DataFrame(encoded,columns=newLabels) # new df \n",
    "\n",
    "    EncodedBostonData.drop(columns=column, inplace=True) # drop old data \n",
    "    EncodedBostonData = pd.concat([EncodedBostonData, dfToAdd], axis=1) # add new columns \n",
    "print(EncodedBostonData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EncodedBostonData['TV']=0\n",
    "EncodedBostonData['Internet']=0\n",
    "EncodedBostonData['Wireless Internet']=0\n",
    "EncodedBostonData['Air Conditioning']=0\n",
    "EncodedBostonData['Kitchen']=0\n",
    "EncodedBostonData['Free Parking on Premises']=0\n",
    "EncodedBostonData['Heating']=0\n",
    "EncodedBostonData['Family/Kid Friendly']=0\n",
    "EncodedBostonData['Smoke Detector']=0\n",
    "EncodedBostonData['Carbon Monoxide Detector']=0\n",
    "EncodedBostonData['Fire Extinguisher']=0\n",
    "EncodedBostonData['Essentials']=0\n",
    "EncodedBostonData['Shampoo']=0\n",
    "EncodedBostonData['Lock on Bedroom Door']=0\n",
    "EncodedBostonData['24-Hour Check-in']=0\n",
    "EncodedBostonData['Hangers']=0\n",
    "EncodedBostonData['Hair Dryer']=0\n",
    "EncodedBostonData['Iron']=0\n",
    "EncodedBostonData['Washer']=0\n",
    "EncodedBostonData['Laptop Friendly Workspace']=0\n",
    "EncodedBostonData['Dryer']=0\n",
    "EncodedBostonData['Smoke Detector']=0\n",
    "EncodedBostonData['Safety Card']=0\n",
    "EncodedBostonData['First Aid Kit']=0\n",
    "EncodedBostonData['Pets Allowed']=0\n",
    "EncodedBostonData['Pets live on this property']=0\n",
    "EncodedBostonData['Dog(s)']=0\n",
    "EncodedBostonData['Gym']=0\n",
    "EncodedBostonData['Smoking Allowed']=0\n",
    "EncodedBostonData['Elevator in Building']=0\n",
    "EncodedBostonData['Cable TV']=0\n",
    "\n",
    "for column in EncodedBostonData[['amenities']]:  \n",
    "  columnSeriesObj = EncodedBostonData[column]\n",
    "  #print('Colunm Name : ', column)\n",
    "  #print('Column Contents : ', columnSeriesObj.index) \n",
    "  k=0\n",
    "    for value in columnSeriesObj:\n",
    "    i=0\n",
    "    for temp in value.split(','):\n",
    "        temp1 = temp.replace(\"\\\"\",\"\").replace(\"{\",\"\").replace(\"}\",\"\")\n",
    "        print(\"1\"+temp1)\n",
    "        i = i+1\n",
    "        if(temp1==\"TV\"):\n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Wireless Internet\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Air Conditioning\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Kitchen\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Free Parking on Premises\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Heating\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Family/Kid Friendly\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Smoke Detector\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Carbon Monoxide Detector\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Fire Extinguisher\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Essentials\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Shampoo\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Lock on Bedroom Door\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"24-Hour Check-in\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Hangers\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Hair Dryer\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Iron\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Washer\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Dryer\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Smoke Detector\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Safety Card\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"First Aid Kit\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Pets Allowed\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Pets live on this property\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Dog(s)\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Gym\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Smoking Allowed\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Elevator in Building\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"Cable TV\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "\n",
    "\n",
    "        #BostonData['newCol'][value.index]=\"hi\"\n",
    "        #BostonData.iloc[value.index,['newCol']] = \"hi\"\n",
    "    #print(i)  \n",
    "    k=k+1\n",
    "\n",
    "list1 = ['TV','Internet', 'Wireless Internet', 'Air Conditioning', 'Kitchen', 'Free Parking on Premises', 'Heating', \n",
    "         'Family/Kid Friendly', 'Smoke Detector', 'Carbon Monoxide Detector', 'Fire Extinguisher', 'Essentials', 'Shampoo',\n",
    "         'Lock on Bedroom Door', '24-Hour Check-in', 'Hangers', 'Hair Dryer', 'Iron', 'Laptop Friendly Workspace',\n",
    "         'Washer','Dryer','Smoke Detector','Safety Card','First Aid Kit','Pets Allowed','Pets live on this property','Dog(s)','Gym',\n",
    "         'Smoking Allowed','Elevator in Building','Cable TV']\n",
    "#print(EncodedBostonData.loc[:,[*list1]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EncodedBostonData['email']=0\n",
    "EncodedBostonData['phone']=0\n",
    "EncodedBostonData['manual_online']=0\n",
    "EncodedBostonData['reviews']=0\n",
    "EncodedBostonData['manual_offline']=0\n",
    "EncodedBostonData['kba']=0\n",
    "EncodedBostonData['facebook']=0\n",
    "EncodedBostonData['jumio']=0\n",
    "EncodedBostonData['amex']=0\n",
    "EncodedBostonData['linkedin']=0\n",
    "EncodedBostonData['google']=0\n",
    "EncodedBostonData['weibo']=0\n",
    "\n",
    "\n",
    "for column in EncodedBostonData[['host_verifications']]:  \n",
    "    columnSeriesObj = EncodedBostonData[column]\n",
    "    #print('Colunm Name : ', column)\n",
    "    #print('Column Contents : ', columnSeriesObj.index) \n",
    "    k=0\n",
    "    for value in columnSeriesObj:\n",
    "    i=0\n",
    "    for temp in value.split(','):\n",
    "        temp1 = temp.replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\")\n",
    "        i = i+1\n",
    "        if(temp1==\"email\"):\n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"phone\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"manual_online\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"reviews\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"manual_offline\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"kba\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"facebook\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"jumio\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"amex\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"linkedin\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"google\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1\n",
    "        elif(temp1==\"weibo\"):  \n",
    "            EncodedBostonData.at[k,temp1]=1     \n",
    "    #print(i)  \n",
    "    k=k+1\n",
    "\n",
    "list1 = ['email','phone', 'manual_online', 'reviews', 'manual_offline', 'kba', 'facebook', \n",
    "         'jumio', 'amex', 'linkedin', 'google', 'weibo']\n",
    "print(EncodedBostonData.loc[:,[*list1]])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove amneities and host_verifications  now that we have replaced it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EncodedBostonData.drop(columns=['amenities'], inplace=True)\n",
    "EncodedBostonData.drop(columns=['host_verifications'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EncodedBostonData.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace NA with means of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for allColumns in EncodedBostonData.columns:\n",
    "    if allColumns == 'security_deposit' or allColumns == 'cleaning_fee':\n",
    "        EncodedBostonData[allColumns].fillna(0, inplace=True)\n",
    "    else:\n",
    "        EncodedBostonData[allColumns].fillna((EncodedBostonData[allColumns].mean()), inplace=True)\n",
    "\n",
    "# Because we one hot encoded, put y at end again. before we did this to EDA \n",
    "cols = list(EncodedBostonData)\n",
    "# move the column to head of list using index, pop and insert\n",
    "cols.insert(2000, cols.pop(cols.index('price')))\n",
    "EncodedBostonData = EncodedBostonData.loc[:, cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, optimizers, callbacks\n",
    "from keras import losses\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(modelName,nfeat,act_func='tanh',optimzer=optimizers.adam(lr=0.1)):\n",
    "    model = Sequential()\n",
    "\n",
    "    if modelName == 'perceptron':\n",
    "        model.add(Dense(8,input_dim=nfeat,activation=act_func, use_bias=True))\n",
    "        model.add(Dense(1))  #1st Hidden Layer - 3 neuron\n",
    "\n",
    "    if modelName == 'neuralnet_3L':\n",
    "        model.add(Dense(5,input_dim=nfeat,activation=act_func, use_bias=True))               #1st Hidden Layer\n",
    "        model.add(Dense(3,activation=act_func))\n",
    "        model.add(Dense(1))                               #Output Layer\n",
    "    if modelName == 'neuralnet_4L':\n",
    "        model.add(Dense(15,input_dim=nfeat,activation=act_func, use_bias=True))               #1st Hidden Layer\n",
    "        model.add(Dense(10,activation=act_func))\n",
    "        model.add(Dense(5,activation=act_func))   #2nd Hidden Layer\n",
    "        model.add(Dense(1, activation=keras.activations.linear))                               #Output Layer             #Output Layer\n",
    "    model.compile(loss=losses.mean_squared_error, optimizer=optimzer)\n",
    "\n",
    "    return model\n",
    "def adj_r2(rSquare, numIndependentVar, numSamples):\n",
    "    return 1-(1-rSquare)*(numSamples -1)/ (numSamples - numIndependentVar - 1)\n",
    "\n",
    "def graphAttsVRVals(rsq, rCv, rbar, numParams, columnName):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(numParams, rsq, label=\"R^2\")\n",
    "    plt.plot( numParams, rCv, label=\"rCv\")\n",
    "    plt.plot(numParams, rbar,  label=\"rAdj\")\n",
    "    plt.xlabel(\"Number params\")\n",
    "    plt.ylabel(\"RVals\")\n",
    "    plt.title(\"RVals for Y={}\".format(columnName))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardSelect( XIndices, colsToUse, xVals, yVals, modelParams, previousRBase):\n",
    "\n",
    "    # param:  XIndices is an array of all incices of the X attributes\n",
    "    # param: colsTouse is ab array of selected columns that the model should use for the base.\n",
    "    # param: modelToUse is the model to predict with.  it should be passed with the object instantiated\n",
    "    initList = XIndices\n",
    "    featuresToUse = colsToUse\n",
    "    # find what index we need to test.  e.i what is not already in the model\n",
    "    featuresToTest = list(set(initList) - set(featuresToUse))\n",
    "\n",
    "    # analyzer will the index of attribute that was tested, and the new r-value the model created with the added attribute\n",
    "    analyzer = pd.Series(index=featuresToTest)\n",
    "\n",
    "    # Get all the columns we need\n",
    "    xInput = xVals.loc[:,[*featuresToUse]]\n",
    "    # have to build\n",
    "    #myBaseModel = buildModel(modelParams['model'], len(xInput.columns), modelParams['activation'], modelParams['optimizer'])\n",
    "    # yVals = yVals.tolist()\n",
    "    #myBaseModel.fit(x=xInput, y=yVals, epochs=modelParams['epochs'], verbose=1)\n",
    "\n",
    "    myBaseRSquare = previousRBase\n",
    "    print(xInput)\n",
    "    print(yVals)\n",
    "    for newColumn in featuresToTest:\n",
    "        # add the feature we want to test to the base X attribtues  we are already using\n",
    "        combinedX = featuresToUse + [newColumn]\n",
    "        # create model with the indices we want to experiment with.\n",
    "\n",
    "        myModel = buildModel(modelParams['model'], len(combinedX), modelParams['activation'], modelParams['optimizer'])\n",
    "        myModel.fit(xVals.iloc[:,[*combinedX]], yVals, epochs=modelParams['epochs'], verbose=1) #modelToUse.fit(xVals.iloc[:,[*combinedX]], yVals)\n",
    "        # add rsquare valeus at location of the index we are experimenting with.\n",
    "\n",
    "        analyzer.loc[newColumn] = r2_score(yVals,myModel.predict(xVals.loc[:,[*combinedX]])) # this gets rsquare value.\n",
    "        print(analyzer)\n",
    "    #if there is a max value in anaylzer.  Let us add that index to the list if the rsquare is better than the current models.\n",
    "    if (analyzer.max()):\n",
    "        if (analyzer.max() > myBaseRSquare):\n",
    "            # index of what column to add that gave best r-value, the actual r-value, rsquar-bar value, and rcv value\n",
    "            return analyzer.idxmax(), analyzer.max()\n",
    "    # else no added features is better than base model\n",
    "    return -1, None\n",
    "\n",
    "def forwardSelectAll(xAtts, yAtts, modelParams):\n",
    "    xIndices = list( (range(0,len(xAtts.columns))))\n",
    "    rSqVals = [-999] # R^2, R^2 Bar, R^2 cv\n",
    "    rCvVals = [0]\n",
    "    rBarVals = [0]\n",
    "    cols = [0] # Question?:  does this value represent the 1's column.\n",
    "    # going to iterate through each value in xIndexArray and pass to forwardSelectMethod to determine rVals\n",
    "    numFeatures = [1]\n",
    "    for i in range(0, len(xAtts.columns)):\n",
    "        myY = yAtts\n",
    "        next_jIdx, next_j = forwardSelect(xIndices, cols, xAtts, myY, modelParams, rSqVals[-1])\n",
    "        if(next_jIdx ==-1):\n",
    "            break # means we found all columns that are significant.\n",
    "        cols.append(next_jIdx)\n",
    "        numFeatures.append(numFeatures[-1]+1)\n",
    "        rSqVals.append(next_j)# calcualte rsquare, rsquarebar, and rcv here.\n",
    "\n",
    "        ## KFOLD R Value\n",
    "        kfold = KFold(n_splits=5, shuffle=True)\n",
    "        cvScoreArray = []\n",
    "        for train, test in kfold.split(xAtts.iloc[:,[*cols]], yAtts):\n",
    "            model = buildModel(modelParams['model'], numFeatures[-1], modelParams['activation'], modelParams['optimizer'])\n",
    "            model.fit(xAtts.iloc[train, [*cols]], yAtts.iloc[train], verbose=0, epochs=modelParams['epochs'])\n",
    "            cvScoreArray.append(r2_score(yAtts.iloc[test], model.predict(xAtts.iloc[test,[*cols]])))\n",
    "        rCvVals.append(sum(cvScoreArray)/len(cvScoreArray))\n",
    "        rBarVals.append(adj_r2(next_j, numFeatures[-1], len(yAtts)))\n",
    "    return cols, numFeatures, rSqVals, rCvVals, rBarVals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNXL Implementation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myOptimizer = optimizers.SGD(learning_rate=.05)\n",
    "\n",
    "myNNXLModelParams = {'model':'neuralnet_4L', 'optimizer':myOptimizer, 'activation': 'sigmoid', 'epochs':200, 'lr':.05}\n",
    "\n",
    "\n",
    "myNNXLData = EncodedBostonData.copy()\n",
    "myNNXLData.columns = list(range(0, len(myNNXLData.columns)))\n",
    "X= myNNXLData.iloc[:,:-1]\n",
    "y= myNNXLData.iloc[:,-1]\n",
    "X = pd.DataFrame(preprocessing.scale(X))\n",
    "y = pd.DataFrame(preprocessing.scale(y.to_numpy().reshape(-1,1)))\n",
    "\n",
    "\n",
    "myCOls, myNumFeat, rVals, rCV, Radj = forwardSelectAll(X, y, myNNXLModelParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#has made rVals index 0 = -999 because we had issues.  \n",
    "rVals.pop(0)\n",
    "rVals.insert(0,0)\n",
    "print(rVals)\n",
    "print(Radj)\n",
    "print(rCV)\n",
    "graphAttsVRVals(rVals, rCV, Radj, myNumFeat, 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myX = X.iloc[:, [*myCOls]]\n",
    "\n",
    "myModel = buildModel(myNNXLModelParams['model'], len(myX.columns), myNNXLModelParams['activation'], optimzer=myNNXLModelParams['optimizer'] )\n",
    "myModel.fit(x=myX, y=y, epochs=200, verbose=0)\n",
    "_, ax = plt.subplots()\n",
    "ax.scatter(x= range(0, len(myX)), y=y, c='blue', label='actual')\n",
    "ax.scatter(x= range(0, len(myX)), y=myModel.predict(myX),c='red', label='predicted' )\n",
    "plt.legend()\n",
    "#plt.plot(myX, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myModel.predict(myX))\n",
    "print(r2_score(y, myModel.predict(myX)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features that were selected:\")\n",
    "print(EncodedSeattleData.columns[[*myCOls]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipped Python Forward Select and Using Scalation ForwardSelected Attributes for NNXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "myOptimizer = optimizers.SGD(learning_rate=.05)\n",
    "myNNXLModelParams = {'model':'neuralnet_4L', 'optimizer':myOptimizer, 'activation': 'sigmoid', 'epochs':200, 'lr':.05}\n",
    "\n",
    "myNNXLData = EncodedBostonData.copy()\n",
    "\n",
    "\n",
    "\n",
    "X= myNNXLData.iloc[:,:-1]\n",
    "X = X.loc[:,['host_response_rate','host_acceptance_rate', 'accommodates', 'beds', 'bathrooms', 'review_scores_rating', 'guests_included', 'minimum_nights']]\n",
    "X.columns = list(range(0, len(X.columns)))\n",
    "y= myNNXLData.iloc[:,-1]\n",
    "myX = pd.DataFrame(preprocessing.scale(X))\n",
    "y = pd.DataFrame(preprocessing.scale(y.to_numpy().reshape(-1,1)))\n",
    "\n",
    "myModel = buildModel(myNNXLModelParams['model'], len(myX.columns), myNNXLModelParams['activation'], optimzer=myNNXLModelParams['optimizer'] )\n",
    "myModel.fit(x=myX, y=y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.scatter(x= range(0, len(myX)), y=y, c='blue', label='actual')\n",
    "ax.scatter(x= range(0, len(myX)), y=myModel.predict(myX),c='red', label='predicted' )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "print(myModel.predict(myX))\n",
    "print('r2')\n",
    "print(r2_score(y, myModel.predict(myX)) )\n",
    "print('r2adj')\n",
    "print(adj_r2(r2_score(y, myModel.predict(myX)), len(myX.columns), len(myX)))\n",
    "cvScoreArray = []\n",
    "for train, test in kfold.split(myX, y):\n",
    "    model = buildModel(myNNXLModelParams['model'], len(myX.columns), myNNXLModelParams['activation'], myNNXLModelParams['optimizer'])\n",
    "    model.fit(myX.iloc[train, :], y.iloc[train], verbose=0, epochs=myNNXLModelParams['epochs'])\n",
    "    cvScoreArray.append(r2_score(y.iloc[test], model.predict(myX.iloc[test,:])))\n",
    "#rCvVals.append(sum(cvScoreArray)/len(cvScoreArray))\n",
    "print(\"RCV:\")\n",
    "print(sum(cvScoreArray)/len(cvScoreArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvScoreArray)\n",
    "print(sum(cvScoreArray)/len(cvScoreArray))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
